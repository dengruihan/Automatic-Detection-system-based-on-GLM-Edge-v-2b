{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.8934426229508197,
  "eval_steps": 1000,
  "global_step": 950,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.040983606557377046,
      "grad_norm": 9.769883155822754,
      "learning_rate": 9.933333333333334e-05,
      "loss": 5.4466,
      "step": 10
    },
    {
      "epoch": 0.08196721311475409,
      "grad_norm": 19.91482162475586,
      "learning_rate": 9.866666666666668e-05,
      "loss": 3.2756,
      "step": 20
    },
    {
      "epoch": 0.12295081967213115,
      "grad_norm": 15.806248664855957,
      "learning_rate": 9.8e-05,
      "loss": 1.5231,
      "step": 30
    },
    {
      "epoch": 0.16393442622950818,
      "grad_norm": 3.568105936050415,
      "learning_rate": 9.733333333333335e-05,
      "loss": 0.738,
      "step": 40
    },
    {
      "epoch": 0.20491803278688525,
      "grad_norm": 5.749304294586182,
      "learning_rate": 9.666666666666667e-05,
      "loss": 0.5342,
      "step": 50
    },
    {
      "epoch": 0.2459016393442623,
      "grad_norm": 8.37830638885498,
      "learning_rate": 9.6e-05,
      "loss": 0.3669,
      "step": 60
    },
    {
      "epoch": 0.28688524590163933,
      "grad_norm": 2.7710371017456055,
      "learning_rate": 9.533333333333334e-05,
      "loss": 0.2721,
      "step": 70
    },
    {
      "epoch": 0.32786885245901637,
      "grad_norm": 5.096784591674805,
      "learning_rate": 9.466666666666667e-05,
      "loss": 0.2233,
      "step": 80
    },
    {
      "epoch": 0.36885245901639346,
      "grad_norm": 3.110245943069458,
      "learning_rate": 9.4e-05,
      "loss": 0.1477,
      "step": 90
    },
    {
      "epoch": 0.4098360655737705,
      "grad_norm": 3.2790286540985107,
      "learning_rate": 9.333333333333334e-05,
      "loss": 0.216,
      "step": 100
    },
    {
      "epoch": 0.45081967213114754,
      "grad_norm": 1.7519479990005493,
      "learning_rate": 9.266666666666666e-05,
      "loss": 0.1231,
      "step": 110
    },
    {
      "epoch": 0.4918032786885246,
      "grad_norm": 1.6271932125091553,
      "learning_rate": 9.200000000000001e-05,
      "loss": 0.1448,
      "step": 120
    },
    {
      "epoch": 0.5327868852459017,
      "grad_norm": 2.6659181118011475,
      "learning_rate": 9.133333333333334e-05,
      "loss": 0.1058,
      "step": 130
    },
    {
      "epoch": 0.5737704918032787,
      "grad_norm": 1.5820502042770386,
      "learning_rate": 9.066666666666667e-05,
      "loss": 0.0998,
      "step": 140
    },
    {
      "epoch": 0.6147540983606558,
      "grad_norm": 1.4795459508895874,
      "learning_rate": 9e-05,
      "loss": 0.0918,
      "step": 150
    },
    {
      "epoch": 0.6557377049180327,
      "grad_norm": 1.1311200857162476,
      "learning_rate": 8.933333333333334e-05,
      "loss": 0.0724,
      "step": 160
    },
    {
      "epoch": 0.6967213114754098,
      "grad_norm": 1.503367304801941,
      "learning_rate": 8.866666666666668e-05,
      "loss": 0.061,
      "step": 170
    },
    {
      "epoch": 0.7377049180327869,
      "grad_norm": 2.3634235858917236,
      "learning_rate": 8.800000000000001e-05,
      "loss": 0.0768,
      "step": 180
    },
    {
      "epoch": 0.7786885245901639,
      "grad_norm": 1.2632938623428345,
      "learning_rate": 8.733333333333333e-05,
      "loss": 0.0657,
      "step": 190
    },
    {
      "epoch": 0.819672131147541,
      "grad_norm": 0.8406507968902588,
      "learning_rate": 8.666666666666667e-05,
      "loss": 0.0972,
      "step": 200
    },
    {
      "epoch": 0.860655737704918,
      "grad_norm": 1.0364031791687012,
      "learning_rate": 8.6e-05,
      "loss": 0.0823,
      "step": 210
    },
    {
      "epoch": 0.9016393442622951,
      "grad_norm": 0.7375250458717346,
      "learning_rate": 8.533333333333334e-05,
      "loss": 0.0591,
      "step": 220
    },
    {
      "epoch": 0.9426229508196722,
      "grad_norm": 1.148598074913025,
      "learning_rate": 8.466666666666667e-05,
      "loss": 0.0499,
      "step": 230
    },
    {
      "epoch": 0.9836065573770492,
      "grad_norm": 2.3792686462402344,
      "learning_rate": 8.4e-05,
      "loss": 0.0574,
      "step": 240
    },
    {
      "epoch": 1.0245901639344261,
      "grad_norm": 3.2642781734466553,
      "learning_rate": 8.333333333333334e-05,
      "loss": 0.0431,
      "step": 250
    },
    {
      "epoch": 1.0655737704918034,
      "grad_norm": 1.1875334978103638,
      "learning_rate": 8.266666666666667e-05,
      "loss": 0.0507,
      "step": 260
    },
    {
      "epoch": 1.1065573770491803,
      "grad_norm": 0.6539525389671326,
      "learning_rate": 8.2e-05,
      "loss": 0.0676,
      "step": 270
    },
    {
      "epoch": 1.1475409836065573,
      "grad_norm": 0.41862499713897705,
      "learning_rate": 8.133333333333334e-05,
      "loss": 0.061,
      "step": 280
    },
    {
      "epoch": 1.1885245901639343,
      "grad_norm": 0.29473602771759033,
      "learning_rate": 8.066666666666667e-05,
      "loss": 0.0409,
      "step": 290
    },
    {
      "epoch": 1.2295081967213115,
      "grad_norm": 0.2608089745044708,
      "learning_rate": 8e-05,
      "loss": 0.0557,
      "step": 300
    },
    {
      "epoch": 1.2704918032786885,
      "grad_norm": 3.33396577835083,
      "learning_rate": 7.933333333333334e-05,
      "loss": 0.07,
      "step": 310
    },
    {
      "epoch": 1.3114754098360657,
      "grad_norm": 2.0210039615631104,
      "learning_rate": 7.866666666666666e-05,
      "loss": 0.0399,
      "step": 320
    },
    {
      "epoch": 1.3524590163934427,
      "grad_norm": 3.0464110374450684,
      "learning_rate": 7.800000000000001e-05,
      "loss": 0.0543,
      "step": 330
    },
    {
      "epoch": 1.3934426229508197,
      "grad_norm": 0.7117337584495544,
      "learning_rate": 7.733333333333333e-05,
      "loss": 0.0565,
      "step": 340
    },
    {
      "epoch": 1.4344262295081966,
      "grad_norm": 0.37363794445991516,
      "learning_rate": 7.666666666666667e-05,
      "loss": 0.0396,
      "step": 350
    },
    {
      "epoch": 1.4754098360655736,
      "grad_norm": 0.33077237010002136,
      "learning_rate": 7.6e-05,
      "loss": 0.0847,
      "step": 360
    },
    {
      "epoch": 1.5163934426229508,
      "grad_norm": 0.87131267786026,
      "learning_rate": 7.533333333333334e-05,
      "loss": 0.0569,
      "step": 370
    },
    {
      "epoch": 1.5573770491803278,
      "grad_norm": 1.7138173580169678,
      "learning_rate": 7.466666666666667e-05,
      "loss": 0.0748,
      "step": 380
    },
    {
      "epoch": 1.598360655737705,
      "grad_norm": 1.0632332563400269,
      "learning_rate": 7.4e-05,
      "loss": 0.0413,
      "step": 390
    },
    {
      "epoch": 1.639344262295082,
      "grad_norm": 0.22736631333827972,
      "learning_rate": 7.333333333333333e-05,
      "loss": 0.0514,
      "step": 400
    },
    {
      "epoch": 1.680327868852459,
      "grad_norm": 3.100588321685791,
      "learning_rate": 7.266666666666667e-05,
      "loss": 0.0537,
      "step": 410
    },
    {
      "epoch": 1.721311475409836,
      "grad_norm": 3.4437668323516846,
      "learning_rate": 7.2e-05,
      "loss": 0.0663,
      "step": 420
    },
    {
      "epoch": 1.762295081967213,
      "grad_norm": 3.3775126934051514,
      "learning_rate": 7.133333333333334e-05,
      "loss": 0.0642,
      "step": 430
    },
    {
      "epoch": 1.8032786885245902,
      "grad_norm": 2.301851749420166,
      "learning_rate": 7.066666666666667e-05,
      "loss": 0.038,
      "step": 440
    },
    {
      "epoch": 1.8442622950819674,
      "grad_norm": 1.0307350158691406,
      "learning_rate": 7e-05,
      "loss": 0.0555,
      "step": 450
    },
    {
      "epoch": 1.8852459016393444,
      "grad_norm": 0.2461206018924713,
      "learning_rate": 6.933333333333334e-05,
      "loss": 0.0545,
      "step": 460
    },
    {
      "epoch": 1.9262295081967213,
      "grad_norm": 1.411241888999939,
      "learning_rate": 6.866666666666666e-05,
      "loss": 0.0497,
      "step": 470
    },
    {
      "epoch": 1.9672131147540983,
      "grad_norm": 2.1138722896575928,
      "learning_rate": 6.800000000000001e-05,
      "loss": 0.0611,
      "step": 480
    },
    {
      "epoch": 2.0081967213114753,
      "grad_norm": 0.1358085721731186,
      "learning_rate": 6.733333333333333e-05,
      "loss": 0.0282,
      "step": 490
    },
    {
      "epoch": 2.0491803278688523,
      "grad_norm": 0.24177607893943787,
      "learning_rate": 6.666666666666667e-05,
      "loss": 0.0443,
      "step": 500
    },
    {
      "epoch": 2.0901639344262297,
      "grad_norm": 2.09329891204834,
      "learning_rate": 6.6e-05,
      "loss": 0.0497,
      "step": 510
    },
    {
      "epoch": 2.1311475409836067,
      "grad_norm": 0.17333705723285675,
      "learning_rate": 6.533333333333334e-05,
      "loss": 0.0419,
      "step": 520
    },
    {
      "epoch": 2.1721311475409837,
      "grad_norm": 2.0096898078918457,
      "learning_rate": 6.466666666666666e-05,
      "loss": 0.0356,
      "step": 530
    },
    {
      "epoch": 2.2131147540983607,
      "grad_norm": 0.5441706776618958,
      "learning_rate": 6.400000000000001e-05,
      "loss": 0.0566,
      "step": 540
    },
    {
      "epoch": 2.2540983606557377,
      "grad_norm": 1.5436371564865112,
      "learning_rate": 6.333333333333333e-05,
      "loss": 0.0485,
      "step": 550
    },
    {
      "epoch": 2.2950819672131146,
      "grad_norm": 0.42946720123291016,
      "learning_rate": 6.266666666666667e-05,
      "loss": 0.0464,
      "step": 560
    },
    {
      "epoch": 2.3360655737704916,
      "grad_norm": 3.250342845916748,
      "learning_rate": 6.2e-05,
      "loss": 0.031,
      "step": 570
    },
    {
      "epoch": 2.3770491803278686,
      "grad_norm": 0.3071412444114685,
      "learning_rate": 6.133333333333334e-05,
      "loss": 0.0385,
      "step": 580
    },
    {
      "epoch": 2.418032786885246,
      "grad_norm": 1.1745957136154175,
      "learning_rate": 6.066666666666667e-05,
      "loss": 0.0462,
      "step": 590
    },
    {
      "epoch": 2.459016393442623,
      "grad_norm": 2.622495174407959,
      "learning_rate": 6e-05,
      "loss": 0.0354,
      "step": 600
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.8487731218338013,
      "learning_rate": 5.9333333333333343e-05,
      "loss": 0.0476,
      "step": 610
    },
    {
      "epoch": 2.540983606557377,
      "grad_norm": 1.8415733575820923,
      "learning_rate": 5.866666666666667e-05,
      "loss": 0.0314,
      "step": 620
    },
    {
      "epoch": 2.581967213114754,
      "grad_norm": 0.2706989645957947,
      "learning_rate": 5.8e-05,
      "loss": 0.0372,
      "step": 630
    },
    {
      "epoch": 2.6229508196721314,
      "grad_norm": 0.3265182375907898,
      "learning_rate": 5.7333333333333336e-05,
      "loss": 0.0397,
      "step": 640
    },
    {
      "epoch": 2.663934426229508,
      "grad_norm": 2.057849407196045,
      "learning_rate": 5.666666666666667e-05,
      "loss": 0.0485,
      "step": 650
    },
    {
      "epoch": 2.7049180327868854,
      "grad_norm": 3.1225128173828125,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 0.0543,
      "step": 660
    },
    {
      "epoch": 2.7459016393442623,
      "grad_norm": 1.1600022315979004,
      "learning_rate": 5.5333333333333334e-05,
      "loss": 0.0478,
      "step": 670
    },
    {
      "epoch": 2.7868852459016393,
      "grad_norm": 0.3307381570339203,
      "learning_rate": 5.466666666666666e-05,
      "loss": 0.0356,
      "step": 680
    },
    {
      "epoch": 2.8278688524590163,
      "grad_norm": 2.088526487350464,
      "learning_rate": 5.4000000000000005e-05,
      "loss": 0.0502,
      "step": 690
    },
    {
      "epoch": 2.8688524590163933,
      "grad_norm": 2.4409875869750977,
      "learning_rate": 5.333333333333333e-05,
      "loss": 0.0331,
      "step": 700
    },
    {
      "epoch": 2.9098360655737707,
      "grad_norm": 1.1059490442276,
      "learning_rate": 5.266666666666666e-05,
      "loss": 0.0431,
      "step": 710
    },
    {
      "epoch": 2.9508196721311473,
      "grad_norm": 0.1291012316942215,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 0.0285,
      "step": 720
    },
    {
      "epoch": 2.9918032786885247,
      "grad_norm": 0.6685484647750854,
      "learning_rate": 5.133333333333333e-05,
      "loss": 0.0539,
      "step": 730
    },
    {
      "epoch": 3.0327868852459017,
      "grad_norm": 2.3000149726867676,
      "learning_rate": 5.0666666666666674e-05,
      "loss": 0.0255,
      "step": 740
    },
    {
      "epoch": 3.0737704918032787,
      "grad_norm": 0.13270798325538635,
      "learning_rate": 5e-05,
      "loss": 0.0251,
      "step": 750
    },
    {
      "epoch": 3.1147540983606556,
      "grad_norm": 0.27903568744659424,
      "learning_rate": 4.933333333333334e-05,
      "loss": 0.0387,
      "step": 760
    },
    {
      "epoch": 3.1557377049180326,
      "grad_norm": 0.1705760657787323,
      "learning_rate": 4.866666666666667e-05,
      "loss": 0.0258,
      "step": 770
    },
    {
      "epoch": 3.19672131147541,
      "grad_norm": 0.5374742150306702,
      "learning_rate": 4.8e-05,
      "loss": 0.0601,
      "step": 780
    },
    {
      "epoch": 3.237704918032787,
      "grad_norm": 0.3329971730709076,
      "learning_rate": 4.7333333333333336e-05,
      "loss": 0.0186,
      "step": 790
    },
    {
      "epoch": 3.278688524590164,
      "grad_norm": 0.32567980885505676,
      "learning_rate": 4.666666666666667e-05,
      "loss": 0.0406,
      "step": 800
    },
    {
      "epoch": 3.319672131147541,
      "grad_norm": 1.6746209859848022,
      "learning_rate": 4.600000000000001e-05,
      "loss": 0.0388,
      "step": 810
    },
    {
      "epoch": 3.360655737704918,
      "grad_norm": 0.12350305169820786,
      "learning_rate": 4.5333333333333335e-05,
      "loss": 0.0319,
      "step": 820
    },
    {
      "epoch": 3.401639344262295,
      "grad_norm": 0.16549503803253174,
      "learning_rate": 4.466666666666667e-05,
      "loss": 0.0229,
      "step": 830
    },
    {
      "epoch": 3.442622950819672,
      "grad_norm": 2.2900238037109375,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 0.0328,
      "step": 840
    },
    {
      "epoch": 3.4836065573770494,
      "grad_norm": 1.1390632390975952,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 0.0269,
      "step": 850
    },
    {
      "epoch": 3.5245901639344264,
      "grad_norm": 0.3886790871620178,
      "learning_rate": 4.266666666666667e-05,
      "loss": 0.0369,
      "step": 860
    },
    {
      "epoch": 3.5655737704918034,
      "grad_norm": 2.798593282699585,
      "learning_rate": 4.2e-05,
      "loss": 0.0521,
      "step": 870
    },
    {
      "epoch": 3.6065573770491803,
      "grad_norm": 3.0188357830047607,
      "learning_rate": 4.133333333333333e-05,
      "loss": 0.0476,
      "step": 880
    },
    {
      "epoch": 3.6475409836065573,
      "grad_norm": 0.5106051564216614,
      "learning_rate": 4.066666666666667e-05,
      "loss": 0.0204,
      "step": 890
    },
    {
      "epoch": 3.6885245901639343,
      "grad_norm": 0.876420795917511,
      "learning_rate": 4e-05,
      "loss": 0.0469,
      "step": 900
    },
    {
      "epoch": 3.7295081967213113,
      "grad_norm": 4.478862285614014,
      "learning_rate": 3.933333333333333e-05,
      "loss": 0.0322,
      "step": 910
    },
    {
      "epoch": 3.7704918032786887,
      "grad_norm": 0.12007798999547958,
      "learning_rate": 3.866666666666667e-05,
      "loss": 0.031,
      "step": 920
    },
    {
      "epoch": 3.8114754098360657,
      "grad_norm": 1.0637500286102295,
      "learning_rate": 3.8e-05,
      "loss": 0.0256,
      "step": 930
    },
    {
      "epoch": 3.8524590163934427,
      "grad_norm": 2.9577133655548096,
      "learning_rate": 3.733333333333334e-05,
      "loss": 0.0468,
      "step": 940
    },
    {
      "epoch": 3.8934426229508197,
      "grad_norm": 0.14140935242176056,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 0.0301,
      "step": 950
    }
  ],
  "logging_steps": 10,
  "max_steps": 1500,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 7,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.309608918720614e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
